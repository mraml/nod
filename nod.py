"""
nod: AI Spec Compliance Gatekeeper

A platform-agnostic, rule-based linter that ensures AI/LLM specifications
contain critical security and compliance elements before automated development.
"""

import argparse, hashlib, hmac, json, os, re, ssl, sys, urllib.request
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
import yaml

class Nod:
    SEVERITY_MAP = {"CRITICAL": 4, "HIGH": 3, "MEDIUM": 2, "LOW": 1, "INFO": 0}
    
    # Mapping for GitHub Code Scanning (0.0-10.0)
    SARIF_SCORE_MAP = {
        "CRITICAL": "9.0", "HIGH": "7.0", "MEDIUM": "5.0",
        "LOW": "3.0", "INFO": "1.0"
    }
    
    SARIF_LEVEL_MAP = {
        "CRITICAL": "error", "HIGH": "error", "MEDIUM": "warning",
        "LOW": "note", "INFO": "note"
    }
    
    DEFAULT_TIMEOUT = 15.0
    MAX_FILE_SIZE = 5 * 1024 * 1024  # 5MB
    MAX_TOTAL_SIZE = 20 * 1024 * 1024 # 20MB

    def __init__(self, rules_sources: List[str], ignore_path: str = ".nodignore") -> None:
        self.config = self._load_rules(rules_sources)
        self.policy_version = self.config.get("version", "unknown")
        self.ignored = self._load_ignore(ignore_path)
        self.attestation: Dict[str, Any] = {}

    def _load_rules(self, sources: List[str]) -> Dict[str, Any]:
        merged = {"profiles": {}, "version": "combined"}
        ctx = ssl.create_default_context()
        ctx.check_hostname = True
        ctx.verify_mode = ssl.CERT_REQUIRED

        def merge(new):
            if not new: return
            for k, v in new.get("profiles", {}).items():
                if k not in merged["profiles"]: merged["profiles"][k] = v
                else: merged["profiles"][k].update(v)

        for src in sources:
            try:
                if src.startswith(("http://", "https://")):
                    with urllib.request.urlopen(src, context=ctx, timeout=self.DEFAULT_TIMEOUT) as r:
                        merge(yaml.safe_load(r.read()))
                elif os.path.isdir(src):
                    for f in sorted(os.listdir(src)):
                        if f.endswith(('.yaml', '.yml')):
                            fp = os.path.join(src, f)
                            if os.path.getsize(fp) > self.MAX_FILE_SIZE:
                                print(f"Warning: Skipping rule {fp} (Size limit)", file=sys.stderr)
                                continue
                            with open(fp, "r", encoding="utf-8") as f_in: merge(yaml.safe_load(f_in))
                elif os.path.exists(src):
                    if os.path.getsize(src) > self.MAX_FILE_SIZE:
                        print(f"Error: Rule file {src} too large", file=sys.stderr); sys.exit(1)
                    with open(src, "r", encoding="utf-8") as f: merge(yaml.safe_load(f))
            except Exception as e: print(f"Error loading rules {src}: {e}", file=sys.stderr); sys.exit(1)
        return merged

    def _load_ignore(self, path: str) -> List[str]:
        if os.path.exists(path):
            try:
                if os.path.getsize(path) <= 1024 * 1024:
                    with open(path, "r", encoding="utf-8") as f:
                        return [l.strip() for l in f if l.strip() and not l.startswith("#")]
            except: pass
        return []

    def _clean(self, text: str) -> str:
        return " ".join(re.sub(r"[#+*?^$\[\](){}|]", "", text).replace(".*", " ").replace(".", " ").split())

    def _resolve_source(self, content: str, index: int) -> str:
        best = "unknown"
        for m in re.finditer(r"<!-- SOURCE: (.*?) -->", content):
            if m.start() < index: best = m.group(1)
            else: break
        return best

    def gen_template(self) -> str:
        lines = ["# AI Project Spec (Generated by nod)", "", f"> Policy: {self.policy_version}\n"]
        for name, data in self.config.get("profiles", {}).items():
            lines.append(f"---\n## Profile: {data.get('badge_label', name)}\n")
            for req in data.get("requirements", []):
                # Use label if available for clearer template
                header = req.get("label") or self._clean(req['id'])
                lines += [f"### {header}", f"<!-- {req.get('remediation', 'Fill section')} -->"]
                if req.get("must_contain"): lines += ["<!-- Subsections: -->"] + req["must_contain"]
                lines.append("TODO: Add details...\n")
        return "\n".join(lines)

    def gen_context(self) -> str:
        lines = ["# SYSTEM COMPLIANCE CONSTRAINTS", f"POLICY: {self.policy_version}", "MANDATORY CONSTRAINTS:\n"]
        for name, data in self.config.get("profiles", {}).items():
            reqs = [r for r in data.get("requirements", []) if r["id"] not in self.ignored]
            flags = [f for f in data.get("red_flags", []) if f["pattern"] not in self.ignored]
            if not reqs and not flags: continue
            lines.append(f"## {data.get('badge_label', name)}")
            if reqs: 
                lines.append("### REQUIRE:")
                for r in reqs:
                    name = r.get("label") or self._clean(r['id'])
                    lines.append(f"- {name}: {r.get('remediation','')}")
            if flags: 
                lines.append("### FORBID:")
                for f in flags: 
                    name = f.get("label") or f"PATTERN '{f['pattern']}'"
                    lines.append(f"- {name}: {f.get('remediation','')}")
            lines.append("")
        return "\n".join(lines)

    def scan_input(self, path: str, strict: bool = False) -> Tuple[Dict, str]:
        files = []
        if os.path.isfile(path): files = [path]
        else:
            for root, dirs, fs in os.walk(path):
                dirs[:] = [d for d in dirs if not d.startswith('.')] # Filter hidden dirs
                files += [os.path.join(root, f) for f in fs if os.path.splitext(f)[1].lower() in {'.md', '.json', '.txt'}]
        
        if not files: return {"error": f"No files in {path}"}, "NONE"
        
        agg, total_sz, hashes, file_map = "", 0, [], {}
        base = path if os.path.isdir(path) else os.path.dirname(path)
        is_single_json = len(files) == 1 and files[0].endswith(".json")

        for fp in files:
            try:
                sz = os.path.getsize(fp)
                if sz > self.MAX_FILE_SIZE: continue
                if (total_sz := total_sz + sz) > self.MAX_TOTAL_SIZE: return {"error": "Total size limit exceeded"}, "NONE"
                with open(fp, "r", encoding="utf-8") as f:
                    raw = f.read()
                    file_map[fp] = raw
                    hashes.append(hashlib.sha256(raw.encode()).hexdigest())
                    agg += raw if is_single_json else f"\n\n<!-- SOURCE: {fp} -->\n{raw}"
            except Exception as e: print(f"Warn: {e}")

        agg_hash = hashlib.sha256("".join(sorted(hashes)).encode()).hexdigest()
        results = self._audit(agg, ".json" if is_single_json else ".md", strict, base, files[0] if is_single_json else None, file_map)
        
        max_sev, max_lab = -1, "NONE"
        for p in results.values():
            for c in p.get("checks", []):
                if not c["passed"] and c["status"] == "FAIL":
                    if (v := self.SEVERITY_MAP.get(c["severity"], 0)) > max_sev: max_sev, max_lab = v, c["severity"]

        self.attestation = {
            "tool": "nod", "version": "1.8.0", "timestamp": datetime.utcnow().isoformat() + "Z",
            "files_audited": files, "aggregate_hash": agg_hash, 
            "max_severity_gap": max_lab, "results": results,
            "remediation_summary": self._gen_prompt(results)
        }
        
        if k := os.environ.get("NOD_SECRET_KEY"):
            p = f"{agg_hash}|{self.attestation['timestamp']}|{max_lab}"
            self.attestation["signature"] = hmac.new(k.encode(), p.encode(), hashlib.sha256).hexdigest()
            self.attestation["signed"] = True
        else: self.attestation["signed"] = False
        
        return results, max_lab

    def _check_req(self, text: str, ext: str, req: Dict, strict: bool) -> Tuple[bool, int, int, str]:
        rid, passed, line, s_idx, err = req["id"], False, 1, -1, ""
        if ext == ".json":
            try:
                d = json.loads(text)
                if rid in d:
                    val = str(d[rid])
                    if not strict or val.strip():
                        passed = True
                        for p in req.get("must_match", []):
                            if p.get("pattern") and not re.search(p["pattern"], val, re.I|re.M):
                                passed, err = False, p.get('message', 'Value mismatch')
            except: pass
        else:
            try:
                if m := re.search(rid, text, re.I|re.M):
                    s_idx, passed, line = m.start(), True, text.count("\n", 0, m.start()) + 1
                    ms = m.group(0).strip()
                    lvl = len(ms) - len(ms.lstrip('#')) if ms.startswith('#') else 0
                    sect = text[m.end():]
                    if nxt := re.search(r"^#{1," + str(lvl) + r"}\s" if lvl else r"^#+\s", sect, re.M): sect = sect[:nxt.start()]
                    
                    if strict and len(sect.strip()) <= 15: passed = False
                    if passed:
                        if miss := [s for s in req.get("must_contain", []) if not re.search(re.escape(s), sect, re.I)]:
                            passed, err = False, f"Missing: {', '.join(miss)}"
                        for p in req.get("must_match", []):
                            if p.get("pattern") and not re.search(p["pattern"], sect, re.I|re.M):
                                passed, err = False, p.get('message', 'Pattern mismatch')
            except: pass
        return passed, line, s_idx, err

    def _audit(self, content: str, ext: str, strict: bool, base: str, def_src: str, fmap: Dict) -> Dict:
        report = {}
        for name, data in self.config.get("profiles", {}).items():
            checks, skip = [], []
            for c in data.get("conditions", []):
                try: 
                    if re.search(c["if"]["regex_match"], content, re.I|re.M): 
                        skip.extend(c["then"].get("skip", []))
                        for r in c["then"].get("require", []):
                            # Append dynamic requirements if needed
                            pass 
                except re.error as e: print(f"Warning: Regex error in condition: {e}", file=sys.stderr)

            for req in data.get("requirements", []):
                rid, stat, passed, ln, src = req["id"], "FAIL", False, 1, def_src
                rem = req.get("remediation", "")
                
                if rid in skip: stat, passed = "SKIPPED", True
                elif rid in self.ignored: stat, passed = "EXCEPTION", True
                else:
                    if req.get("mode") == "in_all_files" and fmap:
                        miss = [os.path.basename(fp) for fp, txt in fmap.items() if not self._check_req(txt, os.path.splitext(fp)[1], req, strict)[0]]
                        if miss: stat, rem = "FAIL", f"Missing in: {', '.join(miss)}. " + rem
                        else: stat, passed, src = "PASS", True, "all_files"
                    else:
                        p, l, idx, e = self._check_req(content, ext, req, strict)
                        if p:
                            stat, passed, ln = "PASS", True, l
                            if not src and idx >= 0: src = self._resolve_source(content, idx)
                        if e: rem = f"{e}. " + rem

                checks.append({
                    "id": rid, "label": req.get("label"), # Store label
                    "passed": passed, "status": stat, "severity": req.get("severity", "HIGH"),
                    "remediation": rem, "source": src, "line": ln, 
                    "control_id": req.get("control_id"), "article": req.get("article")
                })

            for flag in data.get("red_flags", []):
                rid, stat, passed, ln, src = flag["pattern"], "PASS", True, 1, def_src
                try:
                    if m := re.search(rid, content, re.I|re.M):
                        ln = content.count("\n", 0, m.start()) + 1
                        if not src: src = self._resolve_source(content, m.start())
                        if rid in self.ignored: stat = "EXCEPTION"
                        elif rid in skip: stat = "SKIPPED"
                        else: stat, passed = "FAIL", False
                except: pass
                checks.append({
                    "id": rid, "label": flag.get("label"), # Store label
                    "passed": passed, "status": stat, "severity": flag.get("severity", "CRITICAL"),
                    "type": "red_flag", "remediation": flag.get("remediation"),
                    "source": src, "line": ln, "control_id": flag.get("control_id"), "article": flag.get("article")
                })

            # Cross-Refs
            for xr in data.get("cross_references", []):
                try:
                    for m in re.finditer(xr["source"], content, re.I|re.M):
                        exp, ln = m.expand(xr["must_have"]), content.count("\n", 0, m.start()) + 1
                        passed = exp in content
                        checks.append({
                            "id": f"XRef: {m.group(0)}->{exp}", "label": "Cross-Reference Validation",
                            "passed": passed, "status": "PASS" if passed else "FAIL", 
                            "severity": xr.get("severity", "HIGH"), "remediation": f"Missing {exp}",
                            "line": ln, "source": self._resolve_source(content, m.start(), def_src)
                        })
                except: pass

            if strict and ext != ".json" and ("security" in name or "baseline" in name):
                for m in re.finditer(r"\[([^\]]+)\]\((?!http)([^)]+)\)", content):
                    path = m.group(2).strip()
                    if not path.startswith("#"):
                        exists = os.path.exists(os.path.join(base, path))
                        checks.append({
                            "id": f"Ev: {m.group(1)}", "label": "Evidence Check",
                            "passed": exists, "status": "PASS" if exists else "FAIL", 
                            "severity": "MEDIUM", "remediation": f"Missing: {path}",
                            "line": 1, "source": self._resolve_source(content, m.start(), def_src)
                        })

            block = [c for c in checks if c["status"] == "FAIL" and self.SEVERITY_MAP.get(c["severity"], 0) >= 3]
            report[name] = {"label": data.get("badge_label", name), "checks": checks, "passed": not block}
        return report

    def _gen_prompt(self, res: Dict) -> str:
        gaps = []
        for p in res.values():
            for c in p.get("checks", []):
                if c["status"] == "FAIL":
                    # Use label for agent prompt if available
                    name = c.get("label") or c['id']
                    ref = c.get("control_id") or c.get("article") or ""
                    gaps.append(f"- [{c['severity']}] {name} {f'[{ref}]' if ref else ''}: {c.get('remediation', '')}")
        return "\n".join(gaps) if gaps else "No gaps."

    def apply_fix(self, path: str, res: Dict):
        tgt = path if os.path.isfile(path) else os.path.join(path, "nod-compliance.md")
        try:
            with open(tgt, "a", encoding="utf-8") as f:
                f.write("\n\n<!-- nod: auto-fix -->\n")
                cnt = 0
                for data in res.values():
                    miss = [c for c in data["checks"] if c["status"] == "FAIL" and c.get("type") != "red_flag" and not c.get("id").startswith("XRef")]
                    if miss:
                        f.write(f"\n## Missing: {data['label']}\n")
                        for m in miss:
                            # Use label for header if available
                            header = m.get("label") or self._clean(m['id'])
                            f.write(f"\n### {header}\n")
                            if m.get("control_id"): f.write(f"> Ref: {m['control_id']}\n")
                            f.write(f"<!-- {m.get('remediation')} -->\nTODO: Details.\n")
                            cnt += 1
            print(f"âœ… Patched {tgt} (+{cnt})")
        except Exception as e: print(f"Error: {e}")

    def gen_sarif(self, path: str) -> Dict:
        rules, runs, rmap = [], [], {}
        for data in self.attestation["results"].values():
            for c in data["checks"]:
                rid = c["id"]
                if rid not in rmap:
                    rmap[rid] = len(rules)
                    props = {"severity": c["severity"]}
                    if c.get("article"): props["article"] = c["article"]
                    if c.get("control_id"): props["security-severity"] = self.SARIF_SCORE_MAP.get(c["severity"], "1.0")
                    # Use label in shortDescription
                    desc = c.get("label") or rid
                    rules.append({"id": rid, "name": desc, "shortDescription": {"text": c.get("remediation", desc)}, "properties": props})
                
                if c["status"] in ["FAIL", "EXCEPTION"]:
                    uri = c.get("source") if c.get("source") and c.get("source") != "unknown" else path
                    res = {"ruleId": rid, "ruleIndex": rmap[rid], "level": self.SARIF_LEVEL_MAP.get(c["severity"], "note"), "message": {"text": f"Gap: {c.get('remediation')}" if c["status"] == "FAIL" else "Exception"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": uri}, "region": {"startLine": c.get("line", 1)}}}]}
                    if c["status"] == "EXCEPTION": res.update({"kind": "review", "suppressions": [{"kind": "external"}]})
                    runs.append(res)
        return {"version": "2.1.0", "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json", "runs": [{"tool": {"driver": {"name": "nod", "version": self.attestation["version"], "rules": rules}}, "results": runs}]}

    def gen_report(self) -> str:
        out = []
        for data in self.attestation["results"].values():
            chks = data.get("checks", [])
            pct = int((len([c for c in chks if c["status"] != "FAIL"]) / len(chks) * 100) if chks else 0)
            out.append(f"{data['label']} Report ({datetime.utcnow().strftime('%Y-%m-%d')})\nStatus: {pct}% Compliant\n")
            for c in chks:
                icon = {"FAIL": "âŒ", "EXCEPTION": "âšª", "SKIPPED": "â­ï¸"}.get(c["status"], "âœ…")
                ref = c.get("article") or c.get("control_id")
                # Use label instead of ID if available
                name = c.get("label") or self._clean(c['id'])
                out.append(f"{icon} {f'{ref}: ' if ref else ''}{name}")
                if c["status"] == "FAIL": out.append(f"   MISSING: {c.get('remediation','')}")
                elif c["status"] == "PASS" and c.get("source") and c["source"] != "unknown": out.append(f"   Ev: {c['source']}:{c.get('line')}")
                out.append("")
            out.append("-" * 40)
        return "\n".join(out)

def main():
    p = argparse.ArgumentParser(description="nod: AI Spec Compliance")
    p.add_argument("path", nargs="?", help="File/Dir to audit")
    p.add_argument("--rules", action='append')
    p.add_argument("--init", action="store_true")
    p.add_argument("--fix", action="store_true")
    p.add_argument("--export", action="store_true")
    p.add_argument("--strict", action="store_true")
    p.add_argument("--min-severity", default="HIGH", choices=["MEDIUM", "HIGH", "CRITICAL"])
    p.add_argument("--output", choices=["text", "json", "sarif", "compliance"], default="text")
    p.add_argument("--save-to")
    a = p.parse_args()

    defs = ["defaults"] if os.path.isdir("defaults") else ["rules.yaml"]
    scan = Nod(a.rules if a.rules else defs)

    if a.export: print(scan.gen_context()); sys.exit(0)
    if a.init:
        t = scan.gen_template()
        if a.path:
            if os.path.exists(a.path): print("Error: File exists"); sys.exit(1)
            with open(a.path, "w", encoding="utf-8") as f: f.write(t)
            print(f"âœ… Generated: {a.path}")
        else: print(t)
        sys.exit(0)

    if not a.path: p.print_help(); sys.exit(1)
    res, sev = scan.scan_input(a.path, strict=a.strict)
    if a.fix: scan.apply_fix(a.path, res); sys.exit(0)

    out, code = "", 0
    if a.output == "sarif": out = json.dumps(scan.gen_sarif(a.path), indent=2)
    elif a.output == "json": out = json.dumps(scan.attestation, indent=2)
    elif a.output == "compliance": out = scan.gen_report()
    else:
        summ = [f"\n--- nod Summary ---\nTarget: {a.path}\nMax Sev: {sev}"]
        if scan.attestation.get("signed"): summ.append("ðŸ”’ Signed")
        fail, min_v = False, scan.SEVERITY_MAP.get(a.min_severity)
        for d in res.values():
            summ.append(f"\n[{d['label']}]")
            for c in d["checks"]:
                # Use label if available
                name = c.get("label") or c['id']
                if c["status"] == "FAIL":
                    summ.append(f"  âŒ [{c['severity']}] {name}")
                    if c.get("source"): summ.append(f"     File: {c['source']}")
                    if scan.SEVERITY_MAP.get(c["severity"], 0) >= min_v: fail = True
                elif c["status"] == "EXCEPTION": summ.append(f"  âšª [EXCEPTION] {name}")
                elif c["status"] == "SKIPPED": summ.append(f"  â­ï¸  [SKIPPED] {name}")
                else: summ.append(f"  âœ… [PASS] {name}")
        summ.append(f"\nFAIL: Blocked by {a.min_severity}+" if fail else "\nPASS: Nod granted.")
        out = "\n".join(summ)
        if fail: code = 1
        
    if scan.SEVERITY_MAP.get(sev, 0) >= scan.SEVERITY_MAP.get(a.min_severity): code = 1

    if a.save_to:
        try: open(a.save_to, "w", encoding="utf-8").write(out); print(f"Saved: {a.save_to}")
        except Exception as e: print(f"Error: {e}"); sys.exit(1)
    else: print(out)
    sys.exit(code)

if __name__ == "__main__": main()
