"""
nod: AI Spec Compliance Gatekeeper

A platform-agnostic, rule-based linter that ensures AI/LLM specifications
contain critical security and compliance elements before automated development.
"""

import argparse
import hashlib
import hmac  # Added for integrity signing
import json
import os
import re
import ssl
import sys
import urllib.request
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union

import yaml


class Nod:
    """
    The agnostic gatekeeper for AI Spec Compliance.
    Designed to be a 'Policy-as-Code' layer.
    """

    SEVERITY_MAP: Dict[str, int] = {
        "CRITICAL": 4,
        "HIGH": 3,
        "MEDIUM": 2,
        "LOW": 1,
        "INFO": 0,
    }

    SARIF_LEVEL_MAP: Dict[str, str] = {
        "CRITICAL": "error",
        "HIGH": "error",
        "MEDIUM": "warning",
        "LOW": "note",
        "INFO": "note",
    }

    # Security Constants
    DEFAULT_TIMEOUT = 15.0
    MAX_FILE_SIZE = 5 * 1024 * 1024

    def __init__(self, rules_sources: List[str], ignore_path: str = ".nodignore") -> None:
        self.rules_sources = rules_sources
        self.config = self._load_and_merge_rules(rules_sources)
        self.policy_version = self.config.get("version", "unknown")
        self.ignored_rules = self._load_ignore_file(ignore_path)
        self.attestation: Dict[str, Any] = {}

    def _load_and_merge_rules(self, sources: List[str]) -> Dict[str, Any]:
        merged_config = {"profiles": {}, "version": "combined"}
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = True
        ssl_context.verify_mode = ssl.CERT_REQUIRED
        
        for source in sources:
            try:
                if source.startswith(("http://", "https://")):
                    with urllib.request.urlopen(source, context=ssl_context, timeout=self.DEFAULT_TIMEOUT) as response:
                        data = yaml.safe_load(response.read())
                else:
                    if os.path.exists(source) and os.path.getsize(source) > self.MAX_FILE_SIZE:
                        print(f"Error: Rules file {source} exceeds size limit.")
                        sys.exit(1)
                    with open(source, "r", encoding="utf-8") as f:
                        data = yaml.safe_load(f)
                
                if not data: continue
                for profile, content in data.get("profiles", {}).items():
                    if profile not in merged_config["profiles"]:
                        merged_config["profiles"][profile] = content
                    else:
                        target = merged_config["profiles"][profile]
                        target.update(content)
            except Exception as e:
                print(f"Error loading rules from {source}: {str(e)}")
                sys.exit(1)
        return merged_config

    def _load_ignore_file(self, path: str) -> List[str]:
        ignored = []
        if os.path.exists(path):
            try:
                if os.path.getsize(path) > 1024 * 1024: return []
                with open(path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith("#"): ignored.append(line)
            except Exception: pass
        return ignored

    def _get_line_number(self, content: str, index: int) -> int:
        return content.count("\n", 0, index) + 1

    def _clean_header(self, text: str) -> str:
        """Sanitizes regex ID for use as a Markdown header."""
        text = re.sub(r"[#+*?^$\[\](){}|]", "", text).strip()
        text = text.replace(".*", " ").replace(".", " ")
        # Normalize whitespace (replace multiple spaces/tabs with single space)
        return " ".join(text.split())

    def generate_template(self) -> str:
        lines = ["# AI Project Specification (Generated by nod)", "", f"> Policy Version: {self.policy_version}", "> Auto-generated based on compliance requirements.\n"]
        for profile_name, profile_data in self.config.get("profiles", {}).items():
            lines.append(f"---\n## Compliance Profile: {profile_data.get('badge_label', profile_name)}\n")
            for req in profile_data.get("requirements", []):
                lines.append(f"### {self._clean_header(req['id'])}")
                lines.append(f"<!-- {req.get('remediation', 'Fill in this section.')} -->")
                lines.append("TODO: Add details here...\n")
        return "\n".join(lines)

    def generate_system_context(self) -> str:
        lines = ["# SYSTEM COMPLIANCE CONSTRAINTS", f"POLICY VERSION: {self.policy_version}", "The following constraints are MANDATORY.\n"]
        for profile_name, profile_data in self.config.get("profiles", {}).items():
            reqs = [r for r in profile_data.get("requirements", []) if r["id"] not in self.ignored_rules]
            flags = [f for f in profile_data.get("red_flags", []) if f["pattern"] not in self.ignored_rules]
            if not reqs and not flags: continue
            lines.append(f"## PROFILE: {profile_data.get('badge_label', profile_name)}")
            if reqs:
                lines.append("### MUST INCLUDE:")
                for r in reqs: lines.append(f"- {self._clean_header(r['id'])}: {r.get('remediation', '')}")
            if flags:
                lines.append("### FORBIDDEN:")
                for f in flags: lines.append(f"- PATTERN '{f['pattern']}': {f.get('remediation', '')}")
            lines.append("")
        return "\n".join(lines)

    def scan_file(self, file_path: str, strict: bool = False) -> Tuple[Dict[str, Any], str]:
        if not os.path.exists(file_path): return {"error": f"File not found: {file_path}"}, "NONE"
        try:
            if os.path.getsize(file_path) > self.MAX_FILE_SIZE: return {"error": "File too large"}, "NONE"
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                file_hash = hashlib.sha256(content.encode()).hexdigest()
        except Exception as e: return {"error": str(e)}, "NONE"

        results = self._audit_logic(content, os.path.splitext(file_path)[1].lower(), strict, os.path.dirname(file_path))
        
        max_sev_value = -1
        max_sev_label = "NONE"
        for p in results.values():
            for c in p.get("checks", []):
                if not c["passed"] and c["status"] == "FAIL":
                    if self.SEVERITY_MAP.get(c["severity"], 0) > max_sev_value:
                        max_sev_value = self.SEVERITY_MAP.get(c["severity"], 0)
                        max_sev_label = c["severity"]

        self.attestation = {
            "tool": "nod",
            "version": "1.6.0",
            "policy_version": self.policy_version,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "file_sha256": file_hash,
            "max_severity_gap": max_sev_label,
            "results": results,
            "remediation_summary": self._generate_agent_prompt(results),
        }
        
        # Frontier Feature: HMAC Signing
        self._sign_attestation()

        return results, max_sev_label

    def _sign_attestation(self):
        """Cryptographically sign the attestation if NOD_SECRET_KEY is present."""
        secret = os.environ.get("NOD_SECRET_KEY")
        if secret:
            # Create a canonical string representation for signing
            payload = f"{self.attestation['file_sha256']}|{self.attestation['timestamp']}|{self.attestation['max_severity_gap']}"
            signature = hmac.new(secret.encode(), payload.encode(), hashlib.sha256).hexdigest()
            self.attestation["signature"] = signature
            self.attestation["signed"] = True
        else:
            self.attestation["signed"] = False

    def _verify_local_evidence(self, content: str, base_dir: str) -> List[Dict[str, Any]]:
        checks = []
        for match in re.finditer(r"\[([^\]]+)\]\((?!http)([^)]+)\)", content):
            path = match.group(2).strip()
            if path.startswith("#"): continue
            exists = os.path.exists(os.path.join(base_dir, path))
            checks.append({
                "id": f"Evidence: {match.group(1)}", "passed": exists, "status": "PASS" if exists else "FAIL",
                "severity": "MEDIUM", "type": "evidence", "remediation": f"File not found: {path}",
                "line": self._get_line_number(content, match.start())
            })
        return checks

    def _audit_logic(self, content: str, ext: str, strict: bool, base_dir: str) -> Dict[str, Any]:
        report = {}
        for profile, p_data in self.config.get("profiles", {}).items():
            checks = []
            skipped = []
            
            # Conditions
            for cond in p_data.get("conditions", []):
                if "regex_match" in cond.get("if", {}):
                    try:
                        if re.search(cond["if"]["regex_match"], content, re.IGNORECASE | re.MULTILINE):
                            skipped.extend(cond.get("then", {}).get("skip", []))
                    except re.error as e:
                        print(f"‚ö†Ô∏è  Warning: Invalid regex in condition '{cond.get('if', {}).get('regex_match')}': {e}", file=sys.stderr)

            # Requirements
            for req in p_data.get("requirements", []):
                item_id = req["id"]
                status = "FAIL"
                passed = False
                line = 1
                
                if item_id in skipped: status = "SKIPPED"; passed = True
                elif item_id in self.ignored_rules: status = "EXCEPTION"; passed = True
                else:
                    if ext == ".json":
                        try:
                            data = json.loads(content)
                            if item_id in data and (not strict or str(data[item_id]).strip()): passed = True; status = "PASS"
                        except: pass
                    else:
                        try:
                            m = re.search(item_id, content, re.IGNORECASE | re.MULTILINE)
                            if m:
                                line = self._get_line_number(content, m.start())
                                passed = True; status = "PASS"
                                if strict and len(content[m.end():].split("#")[0].strip()) <= 15: passed = False; status = "FAIL"
                        except re.error as e:
                            print(f"‚ö†Ô∏è  Warning: Invalid regex in rule '{item_id}': {e}", file=sys.stderr)
                            # Fail closed on bad regex if it's a security rule
                            status = "FAIL" 

                checks.append({"id": item_id, "passed": passed, "status": status, "severity": req.get("severity", "HIGH").upper(), "remediation": req.get("remediation"), "tags": req.get("tags", []), "line": line})

            # Red Flags
            for flag in p_data.get("red_flags", []):
                item_id = flag["pattern"]
                status = "PASS"; passed = True; line = 1
                try:
                    m = re.search(item_id, content, re.IGNORECASE | re.MULTILINE)
                    if m:
                        line = self._get_line_number(content, m.start())
                        if item_id in self.ignored_rules: status = "EXCEPTION"
                        elif item_id in skipped: status = "SKIPPED"
                        else: status = "FAIL"; passed = False
                except re.error as e:
                    print(f"‚ö†Ô∏è  Warning: Invalid regex in red flag '{item_id}': {e}", file=sys.stderr)
                    # We can't flag it if the regex is broken, but user is warned.
                
                checks.append({"id": item_id, "passed": passed, "status": status, "severity": flag.get("severity", "CRITICAL").upper(), "type": "red_flag", "remediation": flag.get("remediation"), "tags": flag.get("tags", []), "line": line})

            if strict and ext != ".json" and ("security" in profile or "baseline" in profile):
                checks.extend(self._verify_local_evidence(content, base_dir))

            blocking = [c for c in checks if c["status"] == "FAIL" and self.SEVERITY_MAP.get(c["severity"], 0) >= 3]
            report[profile] = {"label": p_data.get("badge_label", profile), "checks": checks, "passed": len(blocking) == 0}
        return report

    def _generate_agent_prompt(self, results: Dict[str, Any]) -> str:
        gaps = []
        for p in results.values():
            for c in p.get("checks", []):
                if c["status"] == "FAIL":
                    tags = f"[{','.join(c.get('tags', []))}]" if c.get('tags') else ""
                    gaps.append(f"- [{c['severity']}] {c['id']} {tags}: {c.get('remediation', '')}")
        return "\n".join(gaps) if gaps else "No gaps detected."

    def apply_fix(self, file_path: str, results: Dict[str, Any]):
        """Appends missing compliance sections to the file."""
        try:
            with open(file_path, "a", encoding="utf-8") as f:
                f.write("\n\n<!-- nod: auto-fix appended below -->\n")
                count = 0
                for p_name, p_data in results.items():
                    missing = [c for c in p_data["checks"] if c["status"] == "FAIL" and c.get("type") != "red_flag"]
                    if not missing: continue
                    
                    # Look up badge label from checks or config? 
                    # We have labels in results.
                    f.write(f"\n## Missing: {p_data['label']}\n")
                    for m in missing:
                        header = self._clean_header(m["id"])
                        f.write(f"\n### {header}\n")
                        f.write(f"<!-- {m.get('remediation')} -->\n")
                        f.write("TODO: Add details here.\n")
                        count += 1
            print(f"‚úÖ patched {file_path}: Appended {count} missing sections.")
        except Exception as e:
            print(f"Error patching file: {e}")

    def generate_sarif(self, file_path: str) -> Dict[str, Any]:
        rules = []; results = []; rule_map = {}
        for p_data in self.attestation["results"].values():
            for c in p_data["checks"]:
                rid = c["id"]
                if rid not in rule_map:
                    rule_map[rid] = len(rules)
                    rules.append({"id": rid, "name": rid, "shortDescription": {"text": c.get("remediation", rid)}, "properties": {"severity": c["severity"], "tags": c.get("tags", [])}})
                
                if c["status"] == "FAIL":
                    results.append({"ruleId": rid, "ruleIndex": rule_map[rid], "level": self.SARIF_LEVEL_MAP.get(c["severity"], "warning"), "message": {"text": f"Gap: {c.get('remediation')}"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": file_path}, "region": {"startLine": c.get("line", 1)}}}]})
                elif c["status"] == "EXCEPTION":
                    results.append({"ruleId": rid, "ruleIndex": rule_map[rid], "level": "note", "kind": "review", "suppressions": [{"kind": "external"}], "message": {"text": "Exception via .nodignore"}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": file_path}, "region": {"startLine": c.get("line", 1)}}}]})
        
        return {"version": "2.1.0", "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json", "runs": [{"tool": {"driver": {"name": "nod", "version": self.attestation["version"], "rules": rules}}, "results": results}]}

def main() -> None:
    parser = argparse.ArgumentParser(description="nod: AI Spec Compliance Gatekeeper")
    parser.add_argument("file", nargs="?", help="The spec file to audit")
    parser.add_argument("--rules", action='append', help="Rule sources")
    parser.add_argument("--init", action="store_true", help="Generate template")
    parser.add_argument("--fix", action="store_true", help="Auto-append missing sections")
    parser.add_argument("--export", action="store_true", help="Export context")
    parser.add_argument("--strict", action="store_true", help="Ensure fields are not empty")
    parser.add_argument("--min-severity", default="HIGH", choices=["MEDIUM", "HIGH", "CRITICAL"])
    parser.add_argument("--output", choices=["text", "json", "sarif"], default="text")
    args = parser.parse_args()

    sources = args.rules if args.rules else ["rules.yaml"]
    scanner = Nod(sources)

    if args.export: print(scanner.generate_system_context()); sys.exit(0)
    
    if args.init:
        template = scanner.generate_template()
        if args.file:
            if os.path.exists(args.file):
                print(f"Error: File '{args.file}' already exists. Aborting to prevent overwrite.")
                sys.exit(1)
            try:
                with open(args.file, "w", encoding="utf-8") as f:
                    f.write(template)
                print(f"‚úÖ Generated compliant spec template: {args.file}")
                sys.exit(0)
            except IOError as e:
                print(f"Error writing template: {e}")
                sys.exit(1)
        else:
            print(template)
            sys.exit(0)

    if not args.file: parser.print_help(); sys.exit(1)

    results, max_sev = scanner.scan_file(args.file, strict=args.strict)

    if args.fix:
        scanner.apply_fix(args.file, results)
        # Re-scan? No, just exit.
        sys.exit(0)

    if args.output == "sarif": print(json.dumps(scanner.generate_sarif(args.file), indent=2))
    elif args.output == "json": print(json.dumps(scanner.attestation, indent=2))
    else:
        print(f"\n--- nod Audit Summary ---")
        print(f"File: {args.file}")
        print(f"Max Severity Gap: {max_sev}")
        if scanner.attestation.get("signed"): print(f"üîí Signature: VERIFIED (HMAC-SHA256)")
        
        failed = False
        min_val = scanner.SEVERITY_MAP.get(args.min_severity)
        for p_data in results.values():
            print(f"\n[{p_data['label']}]")
            for c in p_data["checks"]:
                if c["status"] == "FAIL":
                    icon = "üö©" if c.get("type") == "red_flag" else "‚ùå"
                    print(f"  {icon} [{c['severity']}] {c['id']}")
                    if scanner.SEVERITY_MAP.get(c["severity"], 0) >= min_val: failed = True
                elif c["status"] == "EXCEPTION": print(f"  ‚ö™ [EXCEPTION] {c['id']}")
                elif c["status"] == "SKIPPED": print(f"  ‚è≠Ô∏è  [SKIPPED] {c['id']}")
                else: print(f"  ‚úÖ [PASS] {c['id']}")
        
        if failed: print(f"\nFAIL: Blocked by {args.min_severity}+ gaps."); sys.exit(1)
        print("\nPASS: Nod granted."); sys.exit(0)

if __name__ == "__main__":
    main()
